{
    "child_field": null,
    "class_path": "langchain.embeddings.llama_cpp.LlamaCppEmbeddings",
    "config_schema": {
        "properties": {
            "f16_kv": {
                "default": false,
                "description": "Use half-precision for key/value cache",
                "type": "boolean"
            },
            "logits_all": {
                "default": false,
                "description": "Return logits for all tokens, not just the last token",
                "type": "boolean"
            },
            "model_path": {
                "description": "Path to the Llama model",
                "style": {
                    "width": "100%"
                },
                "type": "string"
            },
            "n_batch": {
                "default": 8,
                "description": "Number of tokens to process in parallel",
                "type": "number"
            },
            "n_ctx": {
                "default": 512,
                "description": "Token context window",
                "type": "number"
            },
            "n_gpu_layers": {
                "description": "Number of layers to be loaded into gpu memory",
                "type": "number"
            },
            "n_parts": {
                "default": -1,
                "description": "Number of parts to split the model into",
                "type": "number"
            },
            "n_threads": {
                "description": "Number of threads to use",
                "type": "number"
            },
            "seed": {
                "default": -1,
                "description": "Seed. If -1, a random seed is used",
                "type": "number"
            },
            "use_mlock": {
                "default": false,
                "description": "Force system to keep model in RAM",
                "type": "boolean"
            },
            "vocab_only": {
                "default": false,
                "description": "Only load the vocabulary, no weights",
                "type": "boolean"
            }
        },
        "required": [],
        "type": "object"
    },
    "connectors": null,
    "description": "LlamaCpp Embeddings",
    "display_type": "node",
    "fields": [
        {
            "description": "Path to the Llama model",
            "name": "model_path",
            "style": {
                "width": "100%"
            },
            "type": "string"
        },
        {
            "default": 512,
            "description": "Token context window",
            "name": "n_ctx",
            "type": "number"
        },
        {
            "default": -1,
            "description": "Number of parts to split the model into",
            "name": "n_parts",
            "type": "number"
        },
        {
            "default": -1,
            "description": "Seed. If -1, a random seed is used",
            "name": "seed",
            "type": "number"
        },
        {
            "default": false,
            "description": "Use half-precision for key/value cache",
            "name": "f16_kv",
            "type": "boolean"
        },
        {
            "default": false,
            "description": "Return logits for all tokens, not just the last token",
            "name": "logits_all",
            "type": "boolean"
        },
        {
            "default": false,
            "description": "Only load the vocabulary, no weights",
            "name": "vocab_only",
            "type": "boolean"
        },
        {
            "default": false,
            "description": "Force system to keep model in RAM",
            "name": "use_mlock",
            "type": "boolean"
        },
        {
            "description": "Number of threads to use",
            "name": "n_threads",
            "type": "number"
        },
        {
            "default": 8,
            "description": "Number of tokens to process in parallel",
            "name": "n_batch",
            "type": "number"
        },
        {
            "description": "Number of layers to be loaded into gpu memory",
            "name": "n_gpu_layers",
            "type": "number"
        }
    ],
    "name": "LlamaCpp Embeddings",
    "type": "embeddings"
}